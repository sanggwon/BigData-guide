## 5. TF-IDF

1. 경로 :

- 모델 생성\03.tfidf_files\make_tfidf.py  

<hr/>
2. TF-IDF 가중치  

- 단어 빈도-역 문서 빈도(Term Frequency- Inverse Document Frequerncy)
- DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법
- 문서의 유사도를 구하는 작업
- 검색 결과의 중요도를 정하는 작업
- 문서 내 특정 단어의 중요도를 구하는 작업

<hr/>
3. 카테고리 분류
```python
category = ['정치', '경제', '사회', '생활', 'IT', '세계', '오피니언', '연예', '스포츠', '기타']
```

<hr/>
4. 언론사, 카테고리, 전체를 구분으로 핵심단어 추출 

```python
if __name__ == "__main__":
    if not os.path.exists(save_path):
        os.mkdir(save_path)

    dir_list = os.listdir(target_path)
    category = ['정치', '경제', '사회', '생활', 'IT', '세계', '오피니언', '연예', '스포츠', '기타']

    all_data = []
    # week_list = {}
    news_data = {}
    for dir in dir_list:
        dir2_list = os.listdir(target_path + '/' + dir)
        for dir2 in dir2_list:
            file_list = os.listdir(target_path + '/' + dir + '/' + dir2)
            for file in file_list:
                fname = target_path + '/' + dir + '/' + dir2 + '/'+ file
                data = pd.read_csv(fname)
                data = pd.DataFrame({'날짜':data.iloc[:,0], '내용':data.iloc[:,1], '구분':data.iloc[:,2]})
                news = file.replace('.csv', '')
                # 언론사별로 구분
                if news not in news_data:
                    news_data[news] = []
                else :
                    news_data[news].append(data)
                # 전체 데이터 저장
                all_data.append(data)

    # 전체 데이터
    data = pd.concat(all_data, axis=0, ignore_index=True, sort = False)
    a_df = data.sample(frac=1).reset_index(drop=True)
    save_json(a_df, '', 'all.csv')

    # 언론사별
    for news in news_data:
        data = pd.concat(news_data[news], axis=0, ignore_index=True, sort = False)
        df = data.sample(frac=1).reset_index(drop=True)
        save_json(df, 'news/', news + '.csv')

    category_len = []
    # 카테고리별
    for i in range(len(category)):
        save_json(a_df[a_df['구분']==i], 'category', category[i] + '.csv')
        category_len.append({category[i] : len(a_df[a_df['구분']==i])})

    with open(save_path + 'count.json', 'w+', encoding='utf-8-sig') as f :
        f.write(str(category_len))
        f.truncate()
        f.close()
```

<hr/>
5. TF-IDF 분석

- 가중치를 부여하여 문서 내 중요 단어를 추출합니다.

```python
def save_json(df, dir, file):
    x = df.iloc[:, 1].values
    word_count_dic = {}
    for doc in x:
        # 단어 COUNT
        word_count(doc, word_count_dic)
    try:
		# min_df : 최소빈도값
		# analyzer : 학습단위를 결정하는 파라미터
		# token_pattern : 기본 토큰 패턴
		# 벡터화
        vector = TfidfVectorizer(min_df=1, analyzer='word', token_pattern=r'\w{1,}')
        # TF-IDF 점수 계산
        sp_matrix = vector.fit_transform(x)
        feature_names = np.array(vector.get_feature_names())
		# 가장 높은 점수를 기준으로 정렬
        max_value = sp_matrix.max(axis=0).toarray().ravel()
        sorted_by_tfidf = max_value.argsort()

        print("tfidf가 가장 낮은 특성 20 개 : \n", feature_names[sorted_by_tfidf[:20]])
        print("tfidf가 가장 높은 특성 20 개 : \n ", feature_names[sorted_by_tfidf[:-20:-1]])

        m =  feature_names[sorted_by_tfidf[::-1]]

        tf_idf_json = {}
        for words in m:
            if words not in tf_idf_json:
                if words in word_count_dic:
                    tf_idf_json[words] = str(word_count_dic[words])

        print(len(tf_idf_json))

        # 저장 경로
        news_dir = save_path + 'news'
        if not os.path.exists(news_dir):
            os.mkdir(news_dir)
        model_dir = save_path + dir
        if not os.path.exists(model_dir):
            os.mkdir(model_dir)
        with open(save_path + dir + '/' + file.replace('csv', 'json'), 'w+', encoding='UTF-8') as f:
            json.dump(tf_idf_json, f, ensure_ascii=False)
    except:
        pass

def word_count(doc, word_count_dic):
    word_list = doc.split()
    for word in word_list:
        if word in word_count_dic:
            word_count_dic[word] += 1
        else:
            word_count_dic[word] = 1
```
