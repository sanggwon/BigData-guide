## 3. 크롤링

1. 경로 : 모델 생성\crawling_files\crawling.py
<hr/>

- 데이터를 수집하기 위한 소스입니다.
- 네이버 뉴스를 기반으로 했습니다.
- 대표 언론사 10곳의 데이터를 수집했습니다.
```Python
newspaper = {'경향신문':'032','국민일보':'005','동아일보':'020','문화일보':'021','서울신문':'081','세계일보':'022','조선일보':'023', '중앙일보':'025', '한겨레':'028', '한국일보':'469'}
```
- 카테고리
```Python
theme_list = ['정치','경제','사회','생활/문화','세계','IT/과학','오피니언']
```
- 프로세스 기반 병렬 처리를 위해 데이터 처리 속도를 개선합니다.
```
pool = multiprocessing.Pool(multiprocessing.cpu_count())
pool.map(page_crawling,newspaper)
pool.close()
pool.join()
```
- 언론사 리스트를 반복문으로 돌려 함수를 실행합니다.
```Python
def page_crawling(news):
    # 시작일,종료일 설정
    last = datetime.now()
	# 년:years, 월:months, 일:days
    start = last - relativedelta(months=1)
    link = {}
    # 언론사 번호
    checked = newspaper[news]
    while start <= last:
        dates = start.strftime("%Y%m%d")
        # 하루 더하기
        start += timedelta(days=1)
        # 10페이지
        for i in range(1, 10):
            webpage = urlopen('https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid='+checked+'&date='+dates+'&page='+str(i))
            soup = BeautifulSoup(webpage,"html.parser",from_encoding="utf-8-sig")
            ul_list = soup.find('div', {'class': 'list_body newsflash_body'}).findAll('ul')
            for ul in ul_list:
                for li in ul.findAll('li'):
                    href = li.find('dt').find('a').attrs['href']
                    ym = datetime.strptime(dates, "%Y%m%d").strftime("%Y%m")
                    if ym not in link:
                        link.update({ym : []})
                    link[ym].append(href)
```


