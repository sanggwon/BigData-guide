## 6. Keras

1. 경로

- 소스("https://github.com/sanggwon/BigData-guide/blob/main/모델%20생성/03.tfidf_files/make_tfidf.py")  

<hr/>
2. 케라스  

- 1. 전처리
- 2. 워드 임베딩
- 3. 모델링
- 4. 컴파일과 훈련
- 5. 평가와 예측
- 6. 모델의 저장

<hr/>
3. GPU 메모리 관리  

```python
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)
```

<hr/>
4. 전처리  

```python
max_word = 7000
max_len = 500

# 단어 집합 생성
tok = Tokenizer(num_words = max_word)
tok.fit_on_texts(x_data)
print(len(tok.word_index))

# 정수 인코딩
sequences = tok.texts_to_sequences(x_data)
print(len(sequences[0]))
print(sequences[0])

# 모든 샘플의 길이를 동일하게 맞춤
sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)
print(len(sequences_matrix))
```

<hr/>
5. train_test_split  

- train/validation 데이터 나누기 
```python
x_train, x_test, y_train, y_test = train_test_split(sequences_matrix, y_data, test_size=0.2)
print(x_train.shape)
```

<hr/>
6. 워드 임베딩(Word Embedding) + 모델링(Modeling)  

```python
x_train, x_test, y_train, y_test = train_test_split(sequences_matrix, y_data, test_size=0.2)
print(x_train.shape)

model_dir = 'train_keras_files/datas'
if not os.path.exists(model_dir):
    os.mkdir(model_dir)
model_path = model_dir + "/keras_lstm.h5"

with tf.device('/device:GPU:0'):
    model = Sequential()
	
    # Embedding
    # max_word : 단어 집합의 크기. 즉, 총 단어의 개수
    # 두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기
    # input_length = 입력 시퀀스의 길이
    model.add(Embedding(max_word, 64, input_length=max_len))
	
    # LSTM
    # 첫번째 인자 : 메모리 셀의 개수
    # return_sequences : 시퀀스 출력 여부
    model.add(LSTM(60, return_sequences=True))
	
    # GlobalMaxPool1D
    # 1D 시간 데이터에 대한 전역 최대 풀링 작업
    model.add(GlobalMaxPool1D())
	
    # model.add(Dropout(0.2))
    # model.add(Dense(50, activation='relu'))
	
    # Dropout
    # 서로 연결된 연결망(layer)에서 0부터 1 사이의 확률로 뉴런을 제거(drop)하는 기법
    # 과도하게 집중하여 학습함으로써 발생할 수 있는 과대적합(Overfitting)을 방지하기 위해 사용
    model.add(Dropout(0.5))

    # Dense
    # 첫번째 인자 : 출력 뉴런의 수를 설정
    # activation : 활성화함수를 설정
    #  - linear : 디폴트 값으로 입력값과 가중치로 계산된 결과 값이 그대로 출력
    #  - sigmoid : 시그모이드 함수로 이진분류에서 출력층에 주로 쓰임
    #  - softmax : 소프드맥스 함수로 다중클래스 분류문제에서 출력층에 주로 쓰임
    #  - relu: Rectified Linear Unit 함수로 은닉층에서 주로 쓰임
    model.add(Dense(nb_classes, activation='softmax'))

    # https://gooopy.tistory.com/90
    # optimizer : 경사 하강법을 어떤 방법으로 사용하지 정함 참고
    # loss : 손실 함수(categorical_crossentropy, 원-핫벡터)
    # metrics : 출력(accuracy, 정확도)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # ModelCheckpoint
    # 콜백 함수
    checkpoint = ModelCheckpoint(filepath=model_path, monitor="val_loss", verbose=1, save_best_only=True)

    # EarlyStopping
    # 조기 종료
    early_stopping = EarlyStopping(monitor='val_loss', patience=3)

model.summary()
```

<hr/>
7. 그래프

```python
hist = model.fit(x_train, y_train, batch_size=500, epochs=20, validation_split=0.2, callbacks=[checkpoint, early_stopping])
print("정확도 : %.4f" % (model.evaluate(x_test, y_test)[1]))

y_vloss = hist.history['val_loss']
y_loss = hist.history['loss']

x_len = np.arange(len(y_loss))

plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')
plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_loss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
plt.grid()
plt.show()

y_vacc = hist.history['val_accuracy']
y_acc = hist.history['accuracy']

x_len = np.arange(len(y_loss))

plt.plot(x_len, y_vacc, marker='.', c='red', label='val_set_acc')
plt.plot(x_len, y_acc, marker='.', c='blue', label='train_set_acc')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('acc')
plt.grid()
plt.show()
```

<hr/>
8. 저장

```python
# saving
with open('train_keras_files/datas/tokenizer.pickle', 'wb') as handle:
    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)
```
