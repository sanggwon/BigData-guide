## 3. 크롤링

1. 경로 : 모델 생성\crawling_files\crawling.py
<hr/>
2. 언론사 리스트
- 데이터를 수집하기 위한 소스입니다.
- 네이버 뉴스를 기반으로 했습니다.
- 대표 언론사 10곳의 데이터를 수집했습니다.
```Python
newspaper = {'경향신문':'032','국민일보':'005','동아일보':'020','문화일보':'021','서울신문':'081','세계일보':'022','조선일보':'023', '중앙일보':'025', '한겨레':'028', '한국일보':'469'}
```
<hr/>
3. 카테고리 리스트
```Python
theme_list = ['정치','경제','사회','생활/문화','세계','IT/과학','오피니언', '연예', '스포츠', '기타']
```
<hr/>
4. retry 설정
- 함수 실행 후, 에러 발생시 얼마나 딜레이 후 함수를 다시 실행할지 정하는 함수입니다.
```Python
def retry(ExceptionToCheck, tries=4, delay=3, backoff=2, logger=None):
    def deco_retry(f):
        @wraps(f)
        def f_retry(*args, **kwargs):
            mtries, mdelay = tries, delay
            while mtries > 1:
                try:
                    return f(*args, **kwargs)
                except Exception as e:
                    msg = "%s, Retrying in %d seconds..." % (str(e), mdelay)
                    if logger:
                        logger.warning(msg)
                    else:
                        print(msg)
                    time.sleep(mdelay)
                    mtries -= 1
                    mdelay *= backoff
            return f(*args, **kwargs)
        return f_retry  # true decorator
    return deco_retry
```
<hr/>
5. URL 오픈
- 크롤링 시, 계속적인 요청으로 인해 서버에서 차단을 요청을 차단합니다. 이를 우회하기 위해 fake_useragent 라이브러리를 사용해 실제 사용자가 접속한 것처럼 요청할 수 있습니다.
- retry 설정합니다. 요청 에러 시, 60초 후에 다시 요청합니다.
```Python
@retry((TimeoutError, URLError, HTTPError), tries=4, delay=60, backoff=2)
def get_link(url) :
    ua=UserAgent()
    rxn_req = Request(url, headers={'User-Agent': ua.chrome})
    return urlopen(rxn_req)
```
<hr/>
6. 크롤링
- 언론사별로 페이지를 검색합니다. (1페이지 당 20개, 10페이지)
- 월별로 분류하여 리스트에 저장합니다.
```python
def page_crawling(news, idx):
    # 시작일,종료일 설정
    last = datetime.now()
    start = last - relativedelta(days=idx)
    link = {}
    # 언론사
    checked = newspaper[news]
    # while start < last:

    dates = start.strftime("%Y%m%d")
    # 하루 더하기
    start += timedelta(days=1)
    # 10페이지
    for i in range(1, 10):
        try :
            url = 'https://news.naver.com/main/list.naver?mode=LPOD&mid=sec&oid='+checked+'&date='+dates+'&page='+str(i)
            webpage = get_link(url)
            soup = BeautifulSoup(webpage,"html.parser",from_encoding="utf-8-sig")
            ul_list = soup.find('div', {'class': 'list_body newsflash_body'}).findAll('ul')
            for ul in ul_list:
                for li in ul.findAll('li'):
                    href = li.find('dt').find('a').attrs['href']
                    ymd = datetime.strptime(dates, "%Y%m%d").strftime("%Y%m%d")
                    if ymd not in link:
                        link.update({ymd : []})
                    link[ymd].append(href)
            webpage.close()
        except :
            continue
```
- 월별로 분류된 리스트를 반복합니다.
- 월별 폴더를 생성합니다.
```python
    # 월별로 반복문
    for ymd in link:
        title, date_list, content, category, url_txt = ['제목'], ['날짜'], ['내용'], ['카테고리'], ['URL']
        # # 저장 경로
        ym_file_path = file_path + ymd[:-2]
        # 저장 경로
        ymd_file_path = ym_file_path + "/" + ymd

        try :
            # 폴더 만들기
            if not os.path.exists(ym_file_path):
                os.mkdir(ym_file_path)
                # time.sleep(180)
        except :
            pass
        try :
            if not os.path.exists(ymd_file_path):
                os.mkdir(ymd_file_path)
        except :
            pass
```
- BeautifulSoup를 사용하여 페이지를 열고, 테마를 분류합니다.
- boilerpipe를 사용하여 페이지에 본문을 인식하여 텍스트로 저장합니다.
```python
        # 반복문 soup
        for url in tqdm(link[ymd]):
            try:
                webpage = get_link(url)
                soup = BeautifulSoup(webpage,"html.parser",from_encoding="utf-8-sig")
                if soup is not None :
                    site = soup.find('meta', {'name': 'twitter:site'})
                    if site is not None :
                        content_attr = site.get_attribute_list('content')[0]
                        if '연예' in content_attr :
                            theme = '연예'
                        elif '스포츠' in content_attr :
                            theme = '스포츠'
                        elif '뉴스' in content_attr :
                            if soup.find('li', {'class': 'is_active'}) is not None :
                                theme = soup.find('li', {'class': 'is_active'}).find('span', {'class':'Nitem_link_menu'}).text
                            else :
                                theme = '기타'
                        else :
                            theme = '기타'
                    else :
                        site = soup.find('meta', {'property': 'og:article:author'})
                        if site is not None :
                            content_attr = site.get_attribute_list('content')[0]
                            if '스포츠' in content_attr :
                                theme = '스포츠'
                            else :
                                theme = '기타'
                        else :
                            theme = '기타'

                    extractor = Extractor(extractor='ArticleExtractor', html=str(soup.find('body')))

                    title_ = soup.find('title').text
                    content_ = str(extractor.getText())
                    date_ = ymd
                    idx = theme_list.index(theme)

                    #테마별
                    title.append(title_)
                    content.append(content_)
                    date_list.append(date_)
                    category.append(str(idx))
                    url_txt.append(url)

                else :
                    print(url)
                webpage.close()
            except Exception as e: # work on python 3.x
                continue
```
- '제목', '날짜', '내용', '카테고리', 'URL'를 엑셀로 작성합니다.
```python
        with open(ymd_file_path + '/' + news + '.csv', 'w+', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            for cnt, i in enumerate(title):
                title_ = title[cnt]
                date_ = date_list[cnt]
                content_ = content[cnt]
                category_ = category[cnt]
                url_ = url_txt[cnt]
                writer.writerow((title_, date_, content_, category_, url_))
            f.truncate()
            f.close()
```
<hr/>
7. multiprocessing
-  multiprocessing를 사용하여 프로세스를 사용하여 병렬처리합니다.
-  언론사가 10개이므로 10을 입력했습니다. 해당 서버에 맞는 프로세서 갯수 이하를 입력해야 합니다.
-  multiprocessing.cpu_count()를 사용하면 해당 서버에 프로세서 갯수를 가져옵니다.
```python
def start(idx) :
    try :
        multiprocessing.freeze_support()
        pool = multiprocessing.Pool(10)
        pool.map(partial(page_crawling, idx=idx), newspaper)
        pool.close()
        pool.join()
    except:
        pass    

if __name__ == "__main__":

    if not os.path.exists(file_path):
        os.mkdir(file_path)

    while True :
        ymd = ''
        file = 'count.txt'
        if os.path.isfile(file):
            with open(file, 'r+', encoding='utf-8-sig') as f: 
                ymd = f.readlines()[0]

        num = 0
        if ymd != '' :
            now  = datetime.now()
            date_to_compare = datetime.strptime(ymd, "%Y%m%d")
            date_diff = now - date_to_compare
            num = date_diff.days

        for i in range(num, num+30) :
            ymd = datetime.now() - relativedelta(days=i)
            ymd = ymd.strftime("%Y%m%d")
            with open('count.txt', 'w+', encoding='utf-8-sig') as f:
                f.write(ymd)
                f.truncate()
                f.close()
            start(i)
```
